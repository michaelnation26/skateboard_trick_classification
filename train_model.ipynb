{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Nuk1P9aIjoca","colab_type":"code","outputId":"964fa43b-4381-4e82-b95f-dc051cbd5ea8","executionInfo":{"status":"ok","timestamp":1551235598102,"user_tz":480,"elapsed":2564,"user":{"displayName":"Michael Nation","photoUrl":"https://lh5.googleusercontent.com/-bIpqbmaPLJc/AAAAAAAAAAI/AAAAAAAAABM/KNhnIL0QRiQ/s64/photo.jpg","userId":"13752494364938472639"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["!git clone https://github.com/dlpbc/keras-kinetics-i3d.git\n","%cd keras-kinetics-i3d"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'keras-kinetics-i3d'...\n","remote: Enumerating objects: 49, done.\u001b[K\n","remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n","Unpacking objects: 100% (49/49), done.\n","/content/keras-kinetics-i3d\n"],"name":"stdout"}]},{"metadata":{"id":"uH_MN7TG48nz","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","    print('User uploaded file \"{name}\" with length {length} bytes'\n","          .format(name=fn, length=len(uploaded[fn])))\n","\n","!unzip data.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DUsCrEsKmg2Z","colab_type":"code","colab":{}},"cell_type":"code","source":["# !wget -P data https://github.com/deepmind/kinetics-i3d/raw/master/data/v_CricketShot_g04_c01_flow.npy\n","# !wget -P data https://github.com/deepmind/kinetics-i3d/raw/master/data/v_CricketShot_g04_c01_rgb.npy\n","# !ls data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KWWb3-1_j4eM","colab_type":"code","outputId":"59eb6c16-f5c1-4d85-dc5d-dd80144dfa57","executionInfo":{"status":"ok","timestamp":1551235750259,"user_tz":480,"elapsed":1906,"user":{"displayName":"Michael Nation","photoUrl":"https://lh5.googleusercontent.com/-bIpqbmaPLJc/AAAAAAAAAAI/AAAAAAAAABM/KNhnIL0QRiQ/s64/photo.jpg","userId":"13752494364938472639"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import cv2\n","import glob\n","import numpy as np\n","import os\n","import random\n","import tensorflow as tf\n","\n","from i3d_inception import Inception_Inflated3d\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.models import Model\n","from keras.utils import to_categorical"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"eZdzEExK5-Jt","colab_type":"code","colab":{}},"cell_type":"code","source":["LABELS = [\n","    \"360_kickflip\",\n","    \"heelflip\",\n","    \"kickflip\", \n","    \"nollie_fakie_360_kickflip\",\n","    \"nollie_fakie_heelflip\",\n","    \"nollie_fakie_kickflip\"\n","]\n","\n","LABEL_IDS = {label: ix for ix, label in enumerate(LABELS)}\n","\n","N_FRAMES = 32\n","FRAME_HEIGHT = 224\n","FRAME_WIDTH = 224\n","N_RGB_CHANNELS = 3\n","N_LABELS = len(LABELS)\n","TRAIN_BATCH_SIZE = 4\n","\n","class DataGenerator:\n","    \n","    def __init__(self, train_dir=None, test_dir=None, shuffle_train=True, validation_split=0.0):\n","        self.frame_h = 224\n","        self.frame_w = 224\n","        \n","        if train_dir:\n","            filepaths = []\n","            sub_dirs_pathname = os.path.join(train_dir, \"*\")\n","            sub_dirs = glob.glob(sub_dirs_pathname)\n","            for sub_dir in sub_dirs:\n","                pathname = os.path.join(sub_dir, \"*\")\n","                filepaths += glob.glob(pathname)\n","            \n","            if shuffle_train:\n","                random.shuffle(filepaths)\n","            \n","            n_train = int((1.0-validation_split) * len(filepaths))\n","            self.train_filepaths = filepaths[:n_train]\n","            self.val_filepaths = filepaths[n_train:]\n","            \n","    def get_train_generator(self):\n","        n_filepaths = len(self.train_filepaths)\n","        while True:\n","            for ix_start in range(0, n_filepaths, TRAIN_BATCH_SIZE):\n","                ix_end = min(ix_start+TRAIN_BATCH_SIZE, n_filepaths)\n","                filepaths_for_batch = self.train_filepaths[ix_start:ix_end]\n","                X, y = self._get_batch(filepaths_for_batch)\n","\n","                yield X, y\n","            \n","    def _get_batch(self, filepaths):\n","        batch_frames = []\n","        batch_labels = []\n","        for filepath in filepaths:\n","            frames = self._get_frames(filepath)\n","            batch_frames.append(frames)\n","            label = self._get_label(filepath)\n","            label_encoded = LABEL_IDS[label]\n","            batch_labels.append(label_encoded)\n","        batch_labels = to_categorical(batch_labels, num_classes=N_LABELS).astype(int)\n","\n","        return np.array(batch_frames), batch_labels\n","        \n","    def _get_frames(self, filepath):\n","        video = cv2.VideoCapture(filepath)\n","        if not video.isOpened():\n","            raise FileNotFoundError(\"The input video path you provided is invalid.\")\n","        \n","        frames = []\n","        while video.isOpened():\n","            grabbed, frame_bgr = video.read()\n","            if not grabbed:\n","                break\n","            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n","            frames.append(frame_rgb)\n","        video.release()\n","        \n","        if len(frames) < N_FRAMES:\n","            n_pad_frames = N_FRAMES - len(frames)\n","            for _ in range(n_pad_frames):\n","                blank_frame = np.zeros((self.frame_h, self.frame_w, 3))\n","                frames.append(blank_frame)\n","        \n","        frames_processed = []\n","        current_ix = 0\n","        step_size = len(frames) / float(N_FRAMES)\n","        for _ in range(N_FRAMES):\n","            frame = cv2.resize(frames[int(current_ix)], (self.frame_h, self.frame_w))\n","            frame = frame / 255.0\n","            frames_processed.append(frame)\n","            current_ix += step_size\n","        \n","        return frames_processed\n","\n","    def _get_label(self, filepath):\n","        return filepath.split(\"/\")[-2]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ALnDTlKj6J0c","colab_type":"code","colab":{}},"cell_type":"code","source":["dg = DataGenerator(train_dir=\"data/train\")\n","train_gen = dg.get_train_generator()\n","\n","# ix = 0\n","# for frames, labels in train_gen:\n","#     print(frames[0].shape, labels[0])\n","    \n","#     ix += 1\n","#     if ix == 5:\n","#         break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"suMspy5Xrht6","colab_type":"code","colab":{}},"cell_type":"code","source":["rgb_model = Inception_Inflated3d(include_top=False, weights='rgb_kinetics_only', \n","                                 input_shape=(N_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, N_RGB_CHANNELS), \n","                                 classes=N_LABELS)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3DotHfvHGqhE","colab_type":"code","colab":{}},"cell_type":"code","source":["for layer in rgb_model.layers:\n","    layer.trainable = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6u5Ri2Ud4ZKp","colab_type":"code","colab":{}},"cell_type":"code","source":["x = rgb_model.output\n","x = Flatten()(x)\n","x = Dense(1024, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","output = Dense(N_LABELS, activation='softmax')(x)\n","\n","model = Model(inputs=rgb_model.input, outputs=output)\n","model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_HgKlj7H7jWB","colab_type":"code","colab":{}},"cell_type":"code","source":["model.fit_generator(train_gen, steps_per_epoch=23, epochs=15)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fQpY-MEmFEql","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}